{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Metadata Generation System - Proof of Concept\n",
    "\n",
    "This notebook demonstrates the core functionality of the automated metadata generation system for documents.\n",
    "\n",
    "## Features Demonstrated:\n",
    "- Document text extraction (PDF, DOCX, TXT)\n",
    "- Semantic content analysis using NLP\n",
    "- Automated metadata generation\n",
    "- Structured JSON output\n",
    "- OCR support (when available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:PDF libraries not available. Install PyPDF2 and pdfplumber for PDF support.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:python-docx not available. Install python-docx for DOCX support.\n",
      "WARNING:root:OCR libraries not available. Install Pillow and pytesseract for OCR support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Import our custom modules\n",
    "from document_processor import DocumentProcessor\n",
    "from metadata_generator import MetadataGenerator\n",
    "from text_extractor import TextExtractor\n",
    "from semantic_analyzer import SemanticAnalyzer\n",
    "from utils import setup_logging\n",
    "from config import SUPPORTED_FORMATS, OUTPUT_DIR\n",
    "\n",
    "# Set up logging\n",
    "logger = setup_logging('INFO')\n",
    "print(\" Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check System Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Document for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sample document created: sample_documents/ai_healthcare_analysis.txt\n",
      "Document length: 1997 characters\n"
     ]
    }
   ],
   "source": [
    "# Create a sample text document for testing\n",
    "sample_text = \"\"\"\n",
    "Artificial Intelligence in Healthcare: A Comprehensive Analysis\n",
    "\n",
    "Introduction:\n",
    "Artificial Intelligence (AI) has emerged as a transformative technology in healthcare, revolutionizing patient care, diagnosis, and treatment methodologies. This comprehensive analysis examines the current applications, benefits, challenges, and future prospects of AI in the healthcare sector.\n",
    "\n",
    "Current Applications:\n",
    "AI technologies are being deployed across various healthcare domains including medical imaging, drug discovery, personalized medicine, and clinical decision support systems. Machine learning algorithms have shown remarkable success in analyzing medical images, detecting diseases like cancer and diabetic retinopathy with accuracy comparable to human specialists.\n",
    "\n",
    "Benefits and Advantages:\n",
    "The implementation of AI in healthcare offers numerous advantages including improved diagnostic accuracy, reduced medical errors, enhanced patient outcomes, and increased operational efficiency. AI-powered systems can process vast amounts of medical data quickly, enabling faster diagnosis and treatment decisions.\n",
    "\n",
    "Challenges and Concerns:\n",
    "Despite its potential, AI adoption in healthcare faces several challenges including data privacy concerns, regulatory compliance, integration with existing systems, and the need for specialized training. Healthcare organizations must address these challenges to successfully implement AI solutions.\n",
    "\n",
    "Conclusion:\n",
    "AI represents a significant opportunity to transform healthcare delivery, improve patient outcomes, and reduce costs. However, successful implementation requires careful consideration of technical, ethical, and regulatory factors. As the technology continues to evolve, collaboration between healthcare professionals, technology experts, and policymakers will be crucial for realizing the full potential of AI in healthcare.\n",
    "\n",
    "Keywords: artificial intelligence, healthcare, machine learning, medical imaging, diagnosis, patient care, digital transformation\n",
    "\"\"\"\n",
    "\n",
    "# Save sample document\n",
    "sample_dir = Path('sample_documents')\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "\n",
    "sample_file = sample_dir / 'ai_healthcare_analysis.txt'\n",
    "with open(sample_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_text.strip())\n",
    "\n",
    "print(f\" Sample document created: {sample_file}\")\n",
    "print(f\"Document length: {len(sample_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Extraction Results:\n",
      "==================================================\n",
      "Success: True\n",
      "Extraction Method: direct_text_read\n",
      "Page Count: 1\n",
      "Word Count: 240\n",
      "Character Count: 1989\n",
      "\n",
      "Extracted Text (first 300 characters):\n",
      "--------------------------------------------------\n",
      "Artificial Intelligence in Healthcare: A Comprehensive Analysis Introduction: Artificial Intelligence (AI) has emerged as a transformative technology in healthcare, revolutionizing patient care, diagnosis, and treatment methodologies. This comprehensive analysis examines the current applications, be...\n"
     ]
    }
   ],
   "source": [
    "# Initialize text extractor\n",
    "text_extractor = TextExtractor()\n",
    "\n",
    "# Extract text from sample document\n",
    "extraction_result = text_extractor.extract_text(sample_file)\n",
    "\n",
    "print(\"Text Extraction Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Success: {extraction_result['success']}\")\n",
    "print(f\"Extraction Method: {extraction_result['extraction_method']}\")\n",
    "print(f\"Page Count: {extraction_result['page_count']}\")\n",
    "print(f\"Word Count: {extraction_result['word_count']}\")\n",
    "print(f\"Character Count: {extraction_result['character_count']}\")\n",
    "\n",
    "# if extraction_result['error']:\n",
    "#     print(f\"Error: {extraction_result['error']}\")\n",
    "\n",
    "print(\"\\nExtracted Text (first 300 characters):\")\n",
    "print(\"-\" * 50)\n",
    "print(extraction_result['text'][:300] + \"...\" if len(extraction_result['text']) > 300 else extraction_result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:semantic_analyzer:Could not load spaCy model: en_core_web_sm\n",
      "ERROR:semantic_analyzer:Install it with: python -m spacy download en_core_web_sm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Analysis Results:\n",
      "==================================================\n",
      "\n",
      " Language: en\n",
      " Sentiment: neutral\n",
      " Readability Score: 0.0/100\n",
      "\n",
      " Text Statistics:\n",
      "  - Sentences: 12\n",
      "  - Words: 240\n",
      "  - Characters: 1989\n",
      "  - Paragraphs: 1\n",
      "  - Avg words/sentence: 20.0\n",
      "\n",
      " Top Keywords:\n",
      "   1. healthcare (freq: 7, score: 0.029)\n",
      "   2. medical (freq: 5, score: 0.021)\n",
      "   3. patient (freq: 4, score: 0.017)\n",
      "   4. artificial (freq: 3, score: 0.013)\n",
      "   5. technology (freq: 3, score: 0.013)\n",
      "   6. including (freq: 3, score: 0.013)\n",
      "   7. challenges (freq: 3, score: 0.013)\n",
      "   8. intelligence (freq: 2, score: 0.008)\n",
      "   9. comprehensive (freq: 2, score: 0.008)\n",
      "  10. analysis (freq: 2, score: 0.008)\n",
      "\n",
      "  Named Entities:\n",
      "  No named entities found (spaCy model may not be available)\n",
      "\n",
      " Topics:\n",
      "  artificial, intelligence, healthcare:, comprehensive, analysis, introduction:, (ai), emerged, transformative, technology\n",
      "\n",
      " Summary:\n",
      "  Artificial Intelligence in Healthcare: A Comprehensive Analysis Introduction: Artificial Intelligence (AI) has emerged as a transformative technology in healthcare, revolutionizing patient care, diagnosis, and treatment methodologies.  This comprehensive analysis examines the current applications, benefits, challenges, and future prospects of AI in the healthcare sector.  Current Applications: AI technologies are being deployed across various healthcare domains including medical imaging, drug discovery, personalized medicine, and clinical decision support systems\n"
     ]
    }
   ],
   "source": [
    "# Initialize semantic analyzer\n",
    "semantic_analyzer = SemanticAnalyzer()\n",
    "\n",
    "# Analyze the extracted text\n",
    "if extraction_result['success'] and extraction_result['text']:\n",
    "    semantic_result = semantic_analyzer.analyze_text(extraction_result['text'])\n",
    "    \n",
    "    print(\"Semantic Analysis Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\n Language: {semantic_result['language']}\")\n",
    "    print(f\" Sentiment: {semantic_result['sentiment']}\")\n",
    "    print(f\" Readability Score: {semantic_result['readability_score']:.1f}/100\")\n",
    "    \n",
    "    print(\"\\n Text Statistics:\")\n",
    "    stats = semantic_result['text_statistics']\n",
    "    print(f\"  - Sentences: {stats['sentence_count']}\")\n",
    "    print(f\"  - Words: {stats['word_count']}\")\n",
    "    print(f\"  - Characters: {stats['character_count']}\")\n",
    "    print(f\"  - Paragraphs: {stats['paragraph_count']}\")\n",
    "    print(f\"  - Avg words/sentence: {stats['average_words_per_sentence']:.1f}\")\n",
    "    \n",
    "    print(\"\\n Top Keywords:\")\n",
    "    for i, keyword in enumerate(semantic_result['keywords'][:10], 1):\n",
    "        print(f\"  {i:2d}. {keyword['word']} (freq: {keyword['frequency']}, score: {keyword['relevance_score']:.3f})\")\n",
    "    \n",
    "    print(\"\\n  Named Entities:\")\n",
    "    if semantic_result['entities']:\n",
    "        for entity in semantic_result['entities'][:10]:\n",
    "            print(f\"  - {entity['text']} ({entity['label']}: {entity['description']})\")\n",
    "    else:\n",
    "        print(\"  No named entities found (spaCy model may not be available)\")\n",
    "    \n",
    "    print(\"\\n Topics:\")\n",
    "    if semantic_result['topics']:\n",
    "        print(f\"  {', '.join(semantic_result['topics'][:10])}\")\n",
    "    else:\n",
    "        print(\"  No topics identified\")\n",
    "    \n",
    "    print(\"\\n Summary:\")\n",
    "    print(f\"  {semantic_result['summary']}\")\n",
    "    \n",
    "else:\n",
    "    print(\" Cannot perform semantic analysis - no text extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Complete Metadata Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:semantic_analyzer:Could not load spaCy model: en_core_web_sm\n",
      "ERROR:semantic_analyzer:Install it with: python -m spacy download en_core_web_sm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Metadata Generation Results:\n",
      "============================================================\n",
      "\n",
      " Document Information:\n",
      "  Filename: ai_healthcare_analysis.txt\n",
      "  File Size: 0.0 MB\n",
      "  Document Type: Text Document\n",
      "\n",
      " Processing Information:\n",
      "  Success: True\n",
      "\n",
      " Text Extraction:\n",
      "  Method: direct_text_read\n",
      "  Word Count: 240\n",
      "  Character Count: 1989\n",
      "  Page Count: 1\n",
      "\n",
      " Derived Metadata:\n",
      "  Title: Ai Healthcare Analysis\n",
      "  Category: Academic/Research\n",
      "  Content Type: Informational\n",
      "  Quality Score: 70.0/100\n",
      "  Complexity: Very Complex\n",
      "  Reading Time: 1 minute\n",
      "  Language: en\n",
      "  Keywords: healthcare, medical, patient, artificial, technology\n",
      "  Topics: artificial, intelligence, healthcare:, comprehensive, analysis\n",
      "  Description: Artificial Intelligence in Healthcare: A Comprehensive Analysis Introduction: Artificial Intelligenc...\n"
     ]
    }
   ],
   "source": [
    "# Initialize metadata generator\n",
    "metadata_generator = MetadataGenerator()\n",
    "\n",
    "# Generate complete metadata\n",
    "metadata = metadata_generator.generate_metadata(sample_file)\n",
    "\n",
    "print(\"Complete Metadata Generation Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Document Information\n",
    "doc_info = metadata.get('document_info', {})\n",
    "print(\"\\n Document Information:\")\n",
    "print(f\"  Filename: {doc_info.get('filename', 'N/A')}\")\n",
    "print(f\"  File Size: {doc_info.get('file_size_mb', 0)} MB\")\n",
    "print(f\"  Document Type: {doc_info.get('document_type', 'N/A')}\")\n",
    "\n",
    "\n",
    "# Processing Information\n",
    "proc_info = metadata.get('processing_info', {})\n",
    "print(\"\\n Processing Information:\")\n",
    "print(f\"  Success: {proc_info.get('success', False)}\")\n",
    "if proc_info.get('errors'):\n",
    "    print(f\"  Errors: {'; '.join(proc_info['errors'])}\")\n",
    "\n",
    "# Extraction Information\n",
    "ext_info = metadata.get('extraction_info', {})\n",
    "print(\"\\n Text Extraction:\")\n",
    "print(f\"  Method: {ext_info.get('extraction_method', 'N/A')}\")\n",
    "print(f\"  Word Count: {ext_info.get('word_count', 0)}\")\n",
    "print(f\"  Character Count: {ext_info.get('character_count', 0)}\")\n",
    "print(f\"  Page Count: {ext_info.get('page_count', 0)}\")\n",
    "\n",
    "# Derived Metadata\n",
    "derived = metadata.get('derived_metadata', {})\n",
    "if derived:\n",
    "    print(\"\\n Derived Metadata:\")\n",
    "    print(f\"  Title: {derived.get('title', 'N/A')}\")\n",
    "    print(f\"  Category: {derived.get('category', 'N/A')}\")\n",
    "    print(f\"  Content Type: {derived.get('content_type', 'N/A')}\")\n",
    "    print(f\"  Quality Score: {derived.get('quality_score', 0)}/100\")\n",
    "    print(f\"  Complexity: {derived.get('complexity_level', 'N/A')}\")\n",
    "    print(f\"  Reading Time: {derived.get('estimated_reading_time', 'N/A')}\")\n",
    "    print(f\"  Language: {derived.get('primary_language', 'N/A')}\")\n",
    "    \n",
    "    if derived.get('top_keywords'):\n",
    "        print(f\"  Keywords: {', '.join(derived['top_keywords'][:5])}\")\n",
    "    \n",
    "    if derived.get('main_topics'):\n",
    "        print(f\"  Topics: {', '.join(derived['main_topics'][:5])}\")\n",
    "    \n",
    "    print(f\"  Description: {derived.get('description', 'N/A')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Metadata to JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Metadata saved to: /Users/tvishadhuper/Downloads/MetadataGenerator/output/ai_healthcare_analysis_metadata.json\n",
      "File size: 6500 bytes\n",
      "\n",
      " JSON Structure:\n",
      "  document_info: {filename, file_extension, file_size_bytes, file_size_mb, created_date...}\n",
      "  extraction_info: {extraction_method, page_count, text, character_count, word_count...}\n",
      "  content_analysis: {keywords, entities, summary, language, sentiment...}\n",
      "  processing_info: {timestamp, version, success, errors}\n",
      "  derived_metadata: {title, description, category, primary_language, quality_score...}\n"
     ]
    }
   ],
   "source": [
    "# Save metadata to JSON file\n",
    "from utils import save_metadata\n",
    "\n",
    "output_file = OUTPUT_DIR / f\"{sample_file.stem}_metadata.json\"\n",
    "success = save_metadata(metadata, output_file)\n",
    "\n",
    "if success:\n",
    "    print(f\" Metadata saved to: {output_file}\")\n",
    "    print(f\"File size: {output_file.stat().st_size} bytes\")\n",
    "    \n",
    "    # Display JSON structure\n",
    "    print(\"\\n JSON Structure:\")\n",
    "    json_keys = list(metadata.keys())\n",
    "    for key in json_keys:\n",
    "        if isinstance(metadata[key], dict):\n",
    "            sub_keys = list(metadata[key].keys())\n",
    "            print(f\"  {key}: {{{', '.join(sub_keys[:5])}{'...' if len(sub_keys) > 5 else ''}}}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(metadata[key]).__name__}\")\n",
    "else:\n",
    "    print(\" Failed to save metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Document Processor (High-level Interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:semantic_analyzer:Could not load spaCy model: en_core_web_sm\n",
      "ERROR:semantic_analyzer:Install it with: python -m spacy download en_core_web_sm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DocumentProcessor for end-to-end processing:\n",
      "=======================================================\n",
      "\n",
      " Processing Statistics:\n",
      "  Total files processed: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize document processor\n",
    "processor = DocumentProcessor(log_level='INFO')\n",
    "\n",
    "# Process the sample document using high-level interface\n",
    "print(\"Using DocumentProcessor for end-to-end processing:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "result_metadata = processor.process_single_document(sample_file)\n",
    "\n",
    "# Get processing statistics\n",
    "stats = processor.get_processing_stats()\n",
    "print(f\"\\n Processing Statistics:\")\n",
    "print(f\"  Total files processed: {stats['total_files']}\")\n",
    "\n",
    "if stats['errors']:\n",
    "    print(f\"  Errors: {'; '.join(stats['errors'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Additional Sample Documents for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: technical_report.txt\n",
      "Created: meeting_minutes.txt\n",
      "Created: policy_document.txt\n",
      "\n",
      " Sample documents directory: /Users/tvishadhuper/Downloads/MetadataGenerator/sample_documents\n",
      "Files created: 4 total\n"
     ]
    }
   ],
   "source": [
    "# Create additional sample documents\n",
    "samples = {\n",
    "    'technical_report.txt': \"\"\"\n",
    "System Performance Analysis Report\n",
    "\n",
    "Executive Summary:\n",
    "This report analyzes the performance metrics of our distributed computing system over the past quarter. The analysis reveals significant improvements in throughput and response times.\n",
    "\n",
    "Methodology:\n",
    "We collected performance data using automated monitoring tools and conducted load testing under various scenarios. The metrics include CPU utilization, memory usage, network latency, and transaction throughput.\n",
    "\n",
    "Key Findings:\n",
    "1. Average response time decreased by 23% compared to the previous quarter\n",
    "2. System throughput increased by 31% during peak hours\n",
    "3. Memory utilization remained stable at 67% average\n",
    "4. Network latency improved by 15% after infrastructure upgrades\n",
    "\n",
    "Recommendations:\n",
    "Based on our analysis, we recommend continued monitoring and potential scaling of the database tier to handle increased load.\n",
    "\"\"\",\n",
    "    \n",
    "    'meeting_minutes.txt': \"\"\"\n",
    "Project Alpha Team Meeting Minutes\n",
    "Date: March 15, 2024\n",
    "Attendees: John Smith, Sarah Johnson, Mike Chen, Lisa Rodriguez\n",
    "\n",
    "Agenda Items Discussed:\n",
    "\n",
    "1. Project Timeline Review\n",
    "   - Current phase completion: 75%\n",
    "   - Next milestone: April 1, 2024\n",
    "   - Risk assessment: Low to medium\n",
    "\n",
    "2. Budget Status\n",
    "   - Expenses to date: $45,000\n",
    "   - Remaining budget: $23,000\n",
    "   - Projected completion cost: $62,000\n",
    "\n",
    "3. Action Items\n",
    "   - John: Complete user interface mockups by March 22\n",
    "   - Sarah: Finalize database schema by March 20\n",
    "   - Mike: Set up testing environment by March 25\n",
    "\n",
    "Next meeting scheduled for March 22, 2024 at 2:00 PM.\n",
    "\"\"\",\n",
    "    \n",
    "    'policy_document.txt': \"\"\"\n",
    "Remote Work Policy - Version 2.1\n",
    "\n",
    "Purpose:\n",
    "This policy establishes guidelines and procedures for employees working remotely to ensure productivity, security, and work-life balance.\n",
    "\n",
    "Scope:\n",
    "This policy applies to all full-time and part-time employees who are approved for remote work arrangements.\n",
    "\n",
    "Eligibility Criteria:\n",
    "- Minimum 6 months of employment\n",
    "- Satisfactory performance reviews\n",
    "- Role suitable for remote work\n",
    "- Reliable internet connection and appropriate workspace\n",
    "\n",
    "Security Requirements:\n",
    "- Use of company-approved VPN for all work-related activities\n",
    "- Regular software updates and security patches\n",
    "- Secure storage of confidential information\n",
    "- Compliance with data protection regulations\n",
    "\n",
    "Performance Expectations:\n",
    "Remote employees are expected to maintain the same level of productivity and communication as office-based employees.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "for filename, content in samples.items():\n",
    "    file_path = sample_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content.strip())\n",
    "    print(f\"Created: {filename}\")\n",
    "\n",
    "print(f\"\\n Sample documents directory: {sample_dir.absolute()}\")\n",
    "print(f\"Files created: {len(samples) + 1} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Processing All Sample Documents:\n",
      "=============================================\n",
      "\n",
      " Batch Processing Results:\n",
      "Total files processed: 8\n",
      "\n",
      " Individual File Results:\n",
      "\n",
      "1. business_report.txt\n",
      "   Title: Business Report\n",
      "   Category: General Document\n",
      "   Quality: 70.0/100\n",
      "   Word Count: 268\n",
      "\n",
      "2. technical_report.txt\n",
      "   Title: Technical Report\n",
      "   Category: Academic/Research\n",
      "   Quality: 70.0/100\n",
      "   Word Count: 119\n",
      "\n",
      "3. policy_document.txt\n",
      "   Title: Policy Document\n",
      "   Category: General Document\n",
      "   Quality: 70.0/100\n",
      "   Word Count: 114\n",
      "\n",
      "4. technical_manual.txt\n",
      "   Title: Technical Manual\n",
      "   Category: Technical/IT\n",
      "   Quality: 70.0/100\n",
      "   Word Count: 295\n",
      "\n",
      "5. ai_research_paper.txt\n",
      "   Title: Ai Research Paper\n",
      "   Category: General Document\n",
      "   Quality: 70.0/100\n",
      "   Word Count: 374\n",
      "\n",
      "6. meeting_minutes.txt\n",
      "   Title: Meeting Minutes\n",
      "   Category: General Document\n",
      "   Quality: 75.0/100\n",
      "   Word Count: 98\n",
      "\n",
      "7. ai_healthcare_analysis.txt\n",
      "   Title: Ai Healthcare Analysis\n",
      "   Category: Academic/Research\n",
      "   Quality: 70.0/100\n",
      "   Word Count: 240\n",
      "\n",
      "8. sample.txt\n",
      "   Title: Sample\n",
      "   Category: Technical/IT\n",
      "   Quality: 70.0/100\n",
      "   Word Count: 120\n",
      "\n",
      "ðŸŽ¯ Final Statistics:\n",
      "  Successfully processed: 8 files\n",
      "  Failed: 0 files\n",
      "  Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processor.reset_stats()\n",
    "\n",
    "\n",
    "print(\"Batch Processing All Sample Documents:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "batch_results = processor.process_directory(sample_dir, recursive=False)\n",
    "\n",
    "print(f\"\\n Batch Processing Results:\")\n",
    "print(f\"Total files processed: {len(batch_results)}\")\n",
    "\n",
    "\n",
    "print(\"\\n Individual File Results:\")\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    doc_info = result.get('document_info', {})\n",
    "    derived = result.get('derived_metadata', {})\n",
    "    processing = result.get('processing_info', {})\n",
    "    \n",
    "    print(f\"\\n{i}. {doc_info.get('filename', 'Unknown')}\")\n",
    "\n",
    "    if processing.get('success', False):\n",
    "        print(f\"   Title: {derived.get('title', 'N/A')}\")\n",
    "        print(f\"   Category: {derived.get('category', 'N/A')}\")\n",
    "        print(f\"   Quality: {derived.get('quality_score', 0)}/100\")\n",
    "        print(f\"   Word Count: {result.get('extraction_info', {}).get('word_count', 0)}\")\n",
    "    else:\n",
    "        errors = processing.get('errors', [])\n",
    "        if errors:\n",
    "            print(f\"   Error: {errors[0]}\")\n",
    "\n",
    "\n",
    "final_stats = processor.get_processing_stats()\n",
    "print(f\"\\nðŸŽ¯ Final Statistics:\")\n",
    "print(f\"  Successfully processed: {final_stats['successful']} files\")\n",
    "print(f\"  Failed: {final_stats['failed']} files\")\n",
    "print(f\"  Success rate: {(final_stats['successful'] / max(1, final_stats['total_files']) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata Comparison Analysis:\n",
      "===================================\n",
      "\n",
      "Filename                  Category             Words    Quality  Readability \n",
      "--------------------------------------------------------------------------------\n",
      "business_report.txt       General Document     268      70.0     20.9        \n",
      "technical_report.txt      Academic/Research    119      70.0     12.8        \n",
      "policy_document.txt       General Document     114      70.0     0.0         \n",
      "technical_manual.txt      Technical/IT         295      70.0     11.6        \n",
      "ai_research_paper.txt     General Document     374      70.0     0.0         \n",
      "meeting_minutes.txt       General Document     98       75.0     46.2        \n",
      "ai_healthcare_analysis.txt Academic/Research    240      70.0     0.0         \n",
      "sample.txt                Technical/IT         120      70.0     14.2        \n",
      "\n",
      "ðŸ“ˆ Summary Statistics:\n",
      "  Average word count: 203.5\n",
      "  Average quality score: 70.6/100\n",
      "  Average readability: 13.2/100\n",
      "\n",
      "ðŸ“Š Document Categories:\n",
      "  General Document: 4 documents\n",
      "  Academic/Research: 2 documents\n",
      "  Technical/IT: 2 documents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Metadata Comparison Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for result in batch_results:\n",
    "    if result.get('processing_info', {}).get('success', False):\n",
    "        doc_info = result.get('document_info', {})\n",
    "        derived = result.get('derived_metadata', {})\n",
    "        extraction = result.get('extraction_info', {})\n",
    "        analysis = result.get('content_analysis', {})\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'filename': doc_info.get('filename', 'Unknown'),\n",
    "            'category': derived.get('category', 'N/A'),\n",
    "            'word_count': extraction.get('word_count', 0),\n",
    "            'quality_score': derived.get('quality_score', 0),\n",
    "            'readability': analysis.get('readability_score', 0),\n",
    "            'complexity': derived.get('complexity_level', 'N/A'),\n",
    "            'reading_time': derived.get('estimated_reading_time', 'N/A'),\n",
    "            'keywords_count': len(analysis.get('keywords', [])),\n",
    "            'entities_count': len(analysis.get('entities', []))\n",
    "        })\n",
    "\n",
    "if comparison_data:\n",
    "    print(f\"\\n{'Filename':<25} {'Category':<20} {'Words':<8} {'Quality':<8} {'Readability':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for data in comparison_data:\n",
    "        print(f\"{data['filename']:<25} {data['category']:<20} {data['word_count']:<8} {data['quality_score']:<8.1f} {data['readability']:<12.1f}\")\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Summary Statistics:\")\n",
    "    avg_words = sum(d['word_count'] for d in comparison_data) / len(comparison_data)\n",
    "    avg_quality = sum(d['quality_score'] for d in comparison_data) / len(comparison_data)\n",
    "    avg_readability = sum(d['readability'] for d in comparison_data) / len(comparison_data)\n",
    "    \n",
    "    print(f\"  Average word count: {avg_words:.1f}\")\n",
    "    print(f\"  Average quality score: {avg_quality:.1f}/100\")\n",
    "    print(f\"  Average readability: {avg_readability:.1f}/100\")\n",
    "    \n",
    "  \n",
    "    categories = {}\n",
    "    for data in comparison_data:\n",
    "        cat = data['category']\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Document Categories:\")\n",
    "    for category, count in categories.items():\n",
    "        print(f\"  {category}: {count} document{'s' if count > 1 else ''}\")\n",
    "else:\n",
    "    print(\"No successful processing results available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Automated Metadata Generation System - POC Summary\n",
      "============================================================\n",
      "\n",
      " Generated Metadata Includes:\n",
      "  â€¢ Document information (size, type, hash, dates)\n",
      "  â€¢ Text extraction details (method, word count, quality)\n",
      "  â€¢ Content analysis (keywords, entities, topics, sentiment)\n",
      "  â€¢ Derived metadata (title, category, description, complexity)\n",
      "  â€¢ Quality metrics (readability, completeness score)\n",
      "\n",
      " Output Files Generated:\n",
      "  â€¢ technical_report_metadata.json (4955 bytes)\n",
      "  â€¢ ai_research_paper_metadata.json (7611 bytes)\n",
      "  â€¢ ai_healthcare_analysis_metadata.json (6500 bytes)\n",
      "  â€¢ business_report_metadata.json (5806 bytes)\n",
      "  â€¢ sample_metadata.json (5071 bytes)\n",
      "  â€¢ technical_manual_metadata.json (5827 bytes)\n",
      "  â€¢ meeting_minutes_metadata.json (4607 bytes)\n",
      "  â€¢ policy_document_metadata.json (4743 bytes)\n",
      "  â€¢ batch_summary_sample_documents.json (2110 bytes)\n"
     ]
    }
   ],
   "source": [
    "print(\" Automated Metadata Generation System - POC Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Generated Metadata Includes:\")\n",
    "print(\"  â€¢ Document information (size, type, hash, dates)\")\n",
    "print(\"  â€¢ Text extraction details (method, word count, quality)\")\n",
    "print(\"  â€¢ Content analysis (keywords, entities, topics, sentiment)\")\n",
    "print(\"  â€¢ Derived metadata (title, category, description, complexity)\")\n",
    "print(\"  â€¢ Quality metrics (readability, completeness score)\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Output Files Generated:\")\n",
    "output_files = list(OUTPUT_DIR.glob('*.json'))\n",
    "for file in output_files:\n",
    "    print(f\"  â€¢ {file.name} ({file.stat().st_size} bytes)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
